{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autoencoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "7H3yTncQfoym"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "h1CiDh7CfqON",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pp-UaomMQJNo"
      },
      "cell_type": "markdown",
      "source": [
        "# Vanilla Autoencoder\n",
        "\n",
        "_Notebook orignially contributed by: [afagarap](https://github.com/afagarap)_\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github.com/tensorflow/examples/blob/master/community/en/autoencoder.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/tree/master/community/en/autoencoder.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NBhbug0Qgmuu"
      },
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "In this notebook, we will create a vanilla autoencoder model using the TensorFlow subclassing API. We are going to use the popular MNIST dataset (Grayscale images of hand-written digits from 0 to 9).\n",
        "\n",
        "We deal with huge amount of data in machine learning which naturally leads to more computations. However, we can pick the parts of the data which contribute the most to a model's learning, thus leading to less computations. The process of choosing the _important_ parts of data is known as _feature selection_, which is among the number of use cases of an _autoencoder_.\n",
        "\n",
        "But what exactly is an autoencoder? Well, let's first recall that a neural network is a computational model that is used for finding a function describing the relationship between data features $x$ and its values or labels $y$, i.e. $y = f(x)$.Â \n",
        "\n",
        "Now, an autoencoder is also a neural network. But instead of finding the function _mapping the features_ $x$ to their _corresponding values or labels_ $y$, it aims to find the function mapping the _features_ $x$ _to their corresponding features_ $x$."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zK2SZnszhSYk"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "Let's start by importing the libraries and functions that we will need."
      ]
    },
    {
      "metadata": {
        "id": "TU96m8RjJ7wY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "S1sepk9uMddm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "print('Is Executing Eagerly?', tf.executing_eagerly())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CH3QqWQ1RiP2"
      },
      "cell_type": "markdown",
      "source": [
        "## Define an autoencoder class\n",
        "\n",
        "An autoencoder consists of two components: (1) an **encoder** which learns the data representation, i.e. _important features_ of input data $x$, and (2) a **decoder** which reconstructs data based on its idea of how it is structured. Mathematically,\n",
        "$$ z = f\\big(h_{e}(x)\\big)$$\n",
        "$$ \\hat{x} = f\\big(h_{d}(z)\\big)$$\n",
        "where $z$ is the learned data representation by encoder $h_{e}$, and $\\hat{x}$ is the reconstructed data by decoder $h_{d}$ based on $z$."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "m3za66lwMjWX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, intermediate_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_layer = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu)\n",
        "        self.output_layer = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu)\n",
        "    \n",
        "    def call(self, input_features):\n",
        "        activation = self.hidden_layer(input_features)\n",
        "        return self.output_layer(activation)\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, intermediate_dim, original_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_layer = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu)\n",
        "        self.output_layer = tf.keras.layers.Dense(units=original_dim, activation=tf.nn.relu)\n",
        "  \n",
        "    def call(self, code):\n",
        "        activation = self.hidden_layer(code)\n",
        "        return self.output_layer(activation)\n",
        "\n",
        "    \n",
        "class Autoencoder(tf.keras.Model):\n",
        "    def __init__(self, intermediate_dim, original_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.loss = []\n",
        "        self.encoder = Encoder(intermediate_dim=intermediate_dim)\n",
        "        self.decoder = Decoder(intermediate_dim=intermediate_dim, original_dim=original_dim)\n",
        "  \n",
        "    def call(self, input_features):\n",
        "        code = self.encoder(input_features)\n",
        "        reconstructed = self.decoder(code)\n",
        "        return reconstructed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "X9NBnWhSnvn4"
      },
      "cell_type": "markdown",
      "source": [
        "## Reconstruction error\n",
        "\n",
        "We only discussed and built the model, but we talked about how it actually learns. All we know up to this point is the _flow of learning_ from the input layer of the encoder which supposedly learns the data representation, and use that representation as input to the decoder that reconstructs the original data.\n",
        "Like \"simple\" neural networks, we an autoencoder learns through [backpropagation](https://www.youtube.com/watch?v=LOc_y67AzCA). However, instead of comparing the values or labels of the model, we compare the reconstructed data and the original data. Let's call this comparison the reconstruction error function, and it is given by the following equation,\n",
        "$$ L = \\dfrac{1}{n} \\sum_{i=0}^{n-1} \\big(\\hat{x}_{i} - x_{i}\\big)^{2}$$\n",
        "where $\\hat{x}$ is the reconstructed data while $x$ is the original data."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Y84daISunw_X",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss(model, original):\n",
        "    reconstruction_error = tf.reduce_mean(tf.square(tf.subtract(model(original), original)))\n",
        "    return reconstruction_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "KVjT6C2SqmH0"
      },
      "cell_type": "markdown",
      "source": [
        "## Forward pass and optimization\n",
        "\n",
        "We will write a function for computing the forward pass, and applying a chosen optimization function."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KNSJqjY7qnCe",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(loss, model, opt, original):\n",
        "    with tf.GradientTape() as tape:\n",
        "        gradients = tape.gradient(loss(model, original), model.trainable_variables)\n",
        "        gradient_variables = zip(gradients, model.trainable_variables)\n",
        "        opt.apply_gradients(gradient_variables)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8c4bNlGLrd5T"
      },
      "cell_type": "markdown",
      "source": [
        "## The training loop\n",
        "\n",
        "Finally, we will write a function to run the training loop. This function will take arguments for the model, the optimization function, the loss, the dataset, and the training epochs.\n",
        "\n",
        "The training loop itself uses a `GradientTape` context defined in `train` for each batch."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "K8Sh1UaQrc5D",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_loop(model, opt, loss, dataset, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for step, batch_features in enumerate(dataset):\n",
        "            train(loss, model, opt, batch_features)\n",
        "            loss_values = loss(model, batch_features)\n",
        "            epoch_loss += loss_values\n",
        "        model.loss.append(epoch_loss)\n",
        "        print('Epoch {}/{}. Loss: {}'.format(epoch + 1, epochs, epoch_loss.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8eCxbz9ZSwjr"
      },
      "cell_type": "markdown",
      "source": [
        "## Process the dataset\n",
        "\n",
        "Now that we have defined our `Autoencoder` class, the loss function, and the training loop, let's import the dataset. We will normalize the pixel values for each example through dividing by maximum pixel value. We shall flatten the examples from 28 by 28 arrays to 784-dimensional vectors."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eAN1ONp6MvI7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_train = x_train / 255.\n",
        "\n",
        "x_train = np.float32(x_train)\n",
        "x_train = np.reshape(x_train, (60000, 784))\n",
        "\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices(x_train).batch(256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "268qdJGGTULP"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "Now all we have to do is instantiate the autoencoder model and choose an optimization function, then pass the intermediate dimension and the original dimension of the images."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "t8nw7mdKMxvb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Autoencoder(intermediate_dim=128, original_dim=784)\n",
        "opt = tf.optimizers.Adam(learning_rate=1e-2)\n",
        "\n",
        "train_loop(model, opt, loss, training_dataset, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VioflTOhTnwl"
      },
      "cell_type": "markdown",
      "source": [
        "## Plot the in-training performance\n",
        "\n",
        "Let's take a look at how the model performed during training in a couple of plots."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "azgmhikhM0EE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(range(10), model.loss)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JccKWvNYTtUW"
      },
      "cell_type": "markdown",
      "source": [
        "## Predictions\n",
        "\n",
        "Finally, we will look at some of the predictions. The wrong predictions are labeled in red color."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-JwpigAlM2dF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "number = 10  # how many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for index in range(number):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, number, index + 1)\n",
        "    plt.imshow(x_test[index].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, number, index + 1 + number)\n",
        "    plt.imshow(model(x_test)[index].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}